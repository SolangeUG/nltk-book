{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Intensity Analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Baby has already been downloaded to ./data/\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import urllib.request, os, gzip\n",
    "\n",
    "datadir = './data/'\n",
    "\n",
    "def download_data(dataset_name, datadir):\n",
    "    filename = 'reviews_%s_5.json' % dataset_name\n",
    "    filepath = os.path.join(datadir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(\"Dataset %s has already been downloaded to %s\" % (dataset_name, datadir))\n",
    "    else:\n",
    "        url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/%s.gz' % filename\n",
    "        urllib.request.urlretrieve(url, filepath + \".gz\")\n",
    "        with gzip.open(filepath + \".gz\", 'rb') as fin:\n",
    "            with open(filepath, 'wb') as fout:\n",
    "                fout.write(fin.read())\n",
    "        print(\"Downloaded dataset %s and saved it to %s\" % (dataset_name, datadir))\n",
    "\n",
    "dataset = \"Baby\"\n",
    "download_data(dataset, datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160792 data for dataset Baby\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(dataset_name, datadir):\n",
    "    filepath = os.path.join(datadir, 'reviews_%s_5.json' % dataset_name)\n",
    "    if not os.path.exists(filepath):\n",
    "        download_data(dataset_name, datadir)\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:                            # read file line by line\n",
    "            item_hash = hash(line)                # we will use this later for partitioning our data \n",
    "            item = json.loads(line)               # convert JSON string to Python dict\n",
    "            item['hash'] = item_hash              # add hash for identification purposes\n",
    "            data.append(item)\n",
    "    print(\"Loaded %d data for dataset %s\" % (len(data), dataset_name))\n",
    "    return data\n",
    "\n",
    "# load the data...\n",
    "baby = load_data(dataset, datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 96102 training examples, 32338 validation examples, and 32352 test examples.\n"
     ]
    }
   ],
   "source": [
    "def partition_train_validation_test(data):\n",
    "    # 60% : modulus is 0, 1, 2, 3, 4, or 5\n",
    "    data_train = [item for item in data if item['hash'] % 10 <= 5]\n",
    "    # 20% : modulus is 6 or 7\n",
    "    data_valid = [item for item in data if item['hash'] % 10 in [6,7]] \n",
    "    # 20% : modulus is 8 or 9\n",
    "    data_test  = [item for item in data if item['hash'] % 10 in [8,9]] \n",
    "    return data_train, data_valid, data_test\n",
    "    \n",
    "baby_train, baby_valid, baby_test = partition_train_validation_test(baby)\n",
    "\n",
    "print(\"We have\", len(baby_train), \"training examples,\", len(baby_valid),\n",
    "      \"validation examples, and\", len(baby_test), \"test examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some positive words: like, empathize, attractively, logical, invigorate, eagerly, positives, succes, outstandingly, invaluable\n",
      "Some negative words: lame, demoralizing, smack, brash, frustrated, anti-white, whore, stall, hung, pillage\n",
      "Words that appear in both sets: enviousness, envious, enviously\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "import random\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "random.seed(1234)\n",
    "print(\"Some positive words:\", \", \".join(random.sample(positive_words, 10)))\n",
    "print(\"Some negative words:\", \", \".join(random.sample(negative_words, 10)))\n",
    "\n",
    "intersection = positive_words & negative_words\n",
    "print(\"Words that appear in both sets: \" + \", \".join(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# English language stop words\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def my_tokenize(text):\n",
    "    \"\"\"\n",
    "    Split text into lower-case tokens, removing all-punctuation tokens and stopwords\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        tokens.extend(x for x in word_tokenize(sentence.lower()) \n",
    "                      if x not in eng_stopwords and any(i.isalpha() for i in x))\n",
    "    return tokens\n",
    "\n",
    "def pos_neg_fraction(text):\n",
    "    \"\"\"\n",
    "    Return the fraction of positive and negative words in a text\n",
    "    \"\"\"\n",
    "    tokens = my_tokenize(text)\n",
    "    count_pos, count_neg = 0, 0\n",
    "    for t in tokens:\n",
    "        if t in positive_words:\n",
    "            count_pos += 1\n",
    "        if t in negative_words:\n",
    "            count_neg += 1\n",
    "    count_all = len(tokens)\n",
    "    if count_all != 0:\n",
    "        return count_pos/count_all, count_neg/count_all\n",
    "    else:\n",
    "        return 0., 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def dataset_to_matrix(data):\n",
    "    \"\"\"Extract our feature matrix from the dataset\"\"\"\n",
    "    return numpy.array([list(pos_neg_fraction(item['reviewText'])) for item in data])\n",
    "\n",
    "X_train = dataset_to_matrix(baby_train)\n",
    "\n",
    "def dataset_to_targets(data):\n",
    "    \"\"\"Extract our target array from the dataset\"\"\"\n",
    "    return numpy.array([item['overall'] for item in data])\n",
    "\n",
    "Y_train = dataset_to_targets(baby_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence = This product wasn't bad.\n",
      "['this', 'product', 'was', \"n't\", 'bad_NEG', '.']\n",
      "Sentence = This is not a bad product.\n",
      "['this', 'is', 'not', 'a_NEG', 'bad_NEG', 'product_NEG', '.']\n",
      "Sentence = This product was bad.\n",
      "['this', 'product', 'was', 'bad', '.']\n",
      "Sentence = This is a bad product.\n",
      "['this', 'is', 'a', 'bad', 'product', '.']\n",
      "I used this for my little on and it was really nice for me as a new mom to have everything so well organized.  There was sufficient space for notes I didn't think, but all in all it was nice to have everything laid out.  If you're organized you could easily make one of these yourself and not spend the money, but when we had our baby I just needed one thing done for me and not have to do it myself and for that it was very helpful.\n",
      "\n",
      "['used', 'little', 'really', 'nice', 'new', 'mom', 'everything', 'well', 'organized', 'sufficient', 'space', 'notes', \"n't\", 'think_NEG', 'nice_NEG', 'everything_NEG', 'laid_NEG', \"'re\", 'organized', 'could', 'easily', 'make', 'one', 'spend_NEG', 'money_NEG', 'baby_NEG', 'needed_NEG', 'one_NEG', 'thing_NEG', 'done_NEG', 'helpful_NEG']\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "examples_negation = [\"This product wasn't bad.\",\n",
    "                     \"This is not a bad product.\",\n",
    "                     \"This product was bad.\",\n",
    "                     \"This is a bad product.\"]\n",
    "\n",
    "for sentence in examples_negation:\n",
    "    tokens_with_negation = mark_negation(word_tokenize(sentence.lower()))\n",
    "    print(\"Sentence =\", sentence)\n",
    "    print(tokens_with_negation)\n",
    "\n",
    "negated_stopwords = set(x + \"_NEG\" for x in eng_stopwords)\n",
    "all_stopwords = eng_stopwords.union(negated_stopwords)        # set union\n",
    "    \n",
    "def tokenize_with_negation(text):\n",
    "    \"\"\"\n",
    "    Split text into lower-case tokens, removing all-punctuation tokens and stopwords\n",
    "    \"\"\" \n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        pretokens = word_tokenize(sentence.lower())\n",
    "        pretokens = [x for x in pretokens if any(i.isalpha() for i in x)]\n",
    "        pretokens = mark_negation(pretokens)\n",
    "        tokens.extend(x for x in pretokens if x not in all_stopwords)\n",
    "    return tokens\n",
    "\n",
    "print(baby_train[31]['reviewText'])\n",
    "print()\n",
    "print(tokenize_with_negation(baby_train[31]['reviewText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8571428571428571, 0.0)\n",
      "(0.0, 0.8571428571428571)\n"
     ]
    }
   ],
   "source": [
    "all_positive_words = positive_words.union({x + \"_NEG\" for x in negative_words})\n",
    "all_negative_words = negative_words.union({x + \"_NEG\" for x in positive_words})\n",
    "\n",
    "def pos_neg_fraction_with_negation(text):\n",
    "    tokens = tokenize_with_negation(text)\n",
    "    # count how many positive and negative words occur in the text\n",
    "    count_pos, count_neg = 0, 0\n",
    "    for t in tokens:\n",
    "        if t in all_positive_words:\n",
    "            count_pos += 1\n",
    "        if t in all_negative_words:\n",
    "            count_neg += 1\n",
    "    count_all = len(tokens)\n",
    "    if count_all != 0:\n",
    "        return count_pos/count_all, count_neg/count_all\n",
    "    else:  # avoid division by zero\n",
    "        return 0., 0.\n",
    "    \n",
    "pos_example = 'This is a good, great, fantastic, amazing, wonderful, super product!!!'\n",
    "neg_example = 'This is a bad, atrocious, terrible, dreadful, awful, abysmal product!!!'\n",
    "print(pos_neg_fraction_with_negation(pos_example))\n",
    "print(pos_neg_fraction_with_negation(neg_example))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_matrix_with_neg(data):\n",
    "    return numpy.array([list(pos_neg_fraction_with_negation(item['reviewText'])) for item in data])\n",
    "\n",
    "X_train_neg = dataset_to_matrix_with_neg(baby_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "it will NOT make a regular octagon, even if you have 8 panelsthe problem is, it  only opens to certain angles, and 135 degrees is not one of themit only does 90, 120, and 150(but 120 degrees is good for a hexagon, just don;t think if you have to have any old number of panels, you can make any polygon you want, you can;t.. additionally the SQUARE, which you can make with the 90 degree angles, are not very strong and not recommended by the instructions, especially if you are thinking, i can have use 8 panels to make a square, it will be very flimsy in the middle of the straight junction.. they specifically say DO NOT do that, have 2 sides joined at a 0 degree angle.. basically a hexagon is about all it really works for.. though a 4 sided square works, it isn;t weak, it is just tiny..)so you can HAVE an 8-sided fencebut the angles will have to be half 120 and half 150 degreeswhich is more of a square, just with bent sidesnot a real regular stop-sign type octagon!@#$plus it;s a little flimsyand (no matter what the shape is) there is no way to have an easy open/close gate, you have to either step over it or take a side apart every time [very hard]when i bought it, the store guy said ''oh what kind of dogs do you have..?''hmALSOwhen your kids get to about 18 months or so, they can either move the sides, by walking while pushing on a side, which changes the angle  you had it set to, or just lifting it and crawling undergood for babiestoddlers, not so mucheither fix it to the floor somehow, or accept that they will escapedogs might do that tooespecially just dig under or root it up with their nose and go underwle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(baby_train[400]['reviewText'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Solange\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "\n",
      "it will NOT make a regular octagon, even if you have 8 panelsthe problem is, it  only opens to certain angles, and 135 degrees is not one of themit only does 90, 120, and 150(but 120 degrees is good for a hexagon, just don;t think if you have to have any old number of panels, you can make any polygon you want, you can;t.. additionally the SQUARE, which you can make with the 90 degree angles, are not very strong and not recommended by the instructions, especially if you are thinking, i can have use 8 panels to make a square, it will be very flimsy in the middle of the straight junction.. they specifically say DO NOT do that, have 2 sides joined at a 0 degree angle.. basically a hexagon is about all it really works for.. though a 4 sided square works, it isn;t weak, it is just tiny..)so you can HAVE an 8-sided fencebut the angles will have to be half 120 and half 150 degreeswhich is more of a square, just with bent sidesnot a real regular stop-sign type octagon!\n",
      "{'neg': 0.057, 'neu': 0.891, 'pos': 0.052, 'compound': -0.441}\n",
      "\n",
      "@#$plus it;s a little flimsyand (no matter what the shape is) there is no way to have an easy open/close gate, you have to either step over it or take a side apart every time [very hard]when i bought it, the store guy said ''oh what kind of dogs do you have..?\n",
      "{'neg': 0.064, 'neu': 0.88, 'pos': 0.055, 'compound': 0.1372}\n",
      "\n",
      "''hmALSOwhen your kids get to about 18 months or so, they can either move the sides, by walking while pushing on a side, which changes the angle  you had it set to, or just lifting it and crawling undergood for babiestoddlers, not so mucheither fix it to the floor somehow, or accept that they will escapedogs might do that tooespecially just dig under or root it up with their nose and go underwle\n",
      "{'neg': 0.0, 'neu': 0.965, 'pos': 0.035, 'compound': 0.3818}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "text = baby_train[400]['reviewText']\n",
    "\n",
    "print()\n",
    "for s in sent_tokenize(text):\n",
    "    print(s)\n",
    "    print(sia.polarity_scores(s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23533333 0.76466667 0.         0.552      1.         0.        ]\n",
      " [0.03833333 0.84266667 0.119      0.115      1.         0.357     ]\n",
      " [0.12725    0.8365     0.03625    0.194      0.865      0.145     ]\n",
      " [0.1595     0.7775     0.063      0.319      0.874      0.126     ]\n",
      " [0.073      0.927      0.         0.219      1.         0.        ]\n",
      " [0.3115     0.6885     0.         0.571      1.         0.        ]\n",
      " [0.172      0.8174     0.0106     0.398      1.         0.053     ]\n",
      " [0.174125   0.825875   0.         0.339      1.         0.        ]\n",
      " [0.0307     0.9106     0.0587     0.122      1.         0.257     ]\n",
      " [0.1035     0.87575    0.02075    0.279      1.         0.096     ]]\n"
     ]
    }
   ],
   "source": [
    "def sia_features(dataset):\n",
    "    \"\"\"For each review text in the dataset, extract:\n",
    "       (1) the mean positive sentiment over all sentences\n",
    "       (2) the mean neutral sentiment over all sentences\n",
    "       (3) the mean negative sentiment over all sentences\n",
    "       (4) the maximum positive sentiment over all sentences\n",
    "       (5) the maximum neutral sentiment over all sentences\n",
    "       (6) the maximum negative sentiment over all sentences\"\"\"\n",
    "    feat_matrix = numpy.empty((len(dataset), 6))\n",
    "    for i in range(len(dataset)):\n",
    "        sentences = sent_tokenize(dataset[i]['reviewText'])\n",
    "        nsent = len(sentences)\n",
    "        if nsent:\n",
    "            sentence_polarities = numpy.empty((nsent, 3))\n",
    "            for j in range(nsent):\n",
    "                polarity = sia.polarity_scores(sentences[j])\n",
    "                sentence_polarities[j, 0] = polarity['pos']\n",
    "                sentence_polarities[j, 1] = polarity['neu']\n",
    "                sentence_polarities[j, 2] = polarity['neg']\n",
    "            feat_matrix[i, 0:3] = numpy.mean(sentence_polarities, axis=0) # mean over the columns\n",
    "            feat_matrix[i, 3:6] = numpy.max(sentence_polarities, axis=0) # maximum over the columns\n",
    "        else:\n",
    "            feat_matrix[i, 0:6] = 0.0\n",
    "    return feat_matrix\n",
    "\n",
    "sia_tr = sia_features(baby_train)\n",
    "print(sia_tr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testmat is:\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]]\n",
      "\n",
      "testmat max is:\n",
      "[ 8.  9. 10. 11.]\n",
      "\n",
      "testmat mean is:\n",
      "[1.5 5.5 9.5]\n"
     ]
    }
   ],
   "source": [
    "testmat = numpy.arange(12.).reshape((3, 4))\n",
    "print(\"testmat is:\")\n",
    "print(testmat)\n",
    "\n",
    "print(\"\\ntestmat max is:\")\n",
    "print(numpy.max(testmat, axis=0))\n",
    "\n",
    "print(\"\\ntestmat mean is:\")\n",
    "print(numpy.mean(testmat, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_features(dataset):\n",
    "    \"\"\"Add two features:\n",
    "       (1) length of review (in thousands of characters) - truncate at 2,500\n",
    "       (2) percentage of exclamation marks (in %)\"\"\"\n",
    "    feat_matrix = numpy.empty((len(dataset), 2))\n",
    "    for i in range(len(dataset)):\n",
    "        text = dataset[i]['reviewText']\n",
    "        feat_matrix[i, 0] = len(text) / 1000.\n",
    "        if text:\n",
    "            feat_matrix[i, 1] = 100. * text.count('!') / len(text)\n",
    "        else:\n",
    "            feat_matrix[i, 1] = 0.0\n",
    "    feat_matrix[feat_matrix>2.5] = 2.5\n",
    "    return feat_matrix\n",
    "\n",
    "len_tr = len_features(baby_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96102, 2) (96102, 6) (96102, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_neg.shape, sia_tr.shape, len_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the mean absolute error on the training data is 0.753687 stars\n"
     ]
    }
   ],
   "source": [
    "# stack horizontally\n",
    "X_train_augmented = numpy.concatenate((X_train_neg, sia_tr, len_tr), axis=1) \n",
    "lreg_augmented = LinearRegression().fit(X_train_augmented, Y_train)\n",
    "pred_train_augmented = lreg_augmented.predict(X_train_augmented)\n",
    "mae_train_augmented = mean_absolute_error(pred_train_augmented, Y_train)\n",
    "print(\"Now the mean absolute error on the training data is %f stars\" % mae_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the RF, it is 0.281091 stars\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_augmented = RandomForestRegressor().fit(X_train_augmented, Y_train)\n",
    "rfpred_train_augmented = rf_augmented.predict(X_train_augmented)\n",
    "mae_train_rf_augmented = mean_absolute_error(rfpred_train_augmented, Y_train)\n",
    "print(\"For the RF, it is %f stars\" % mae_train_rf_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the validation set, we get 0.758383 error for the linear regression\n",
      "And 0.733395 for the random forest regression\n"
     ]
    }
   ],
   "source": [
    "X_valid_neg = dataset_to_matrix_with_neg(baby_valid)\n",
    "Y_valid = dataset_to_targets(baby_valid)\n",
    "\n",
    "sia_valid = sia_features(baby_valid)\n",
    "len_valid = len_features(baby_valid)\n",
    "\n",
    "X_valid_augmented = numpy.concatenate((X_valid_neg, sia_valid, len_valid), axis=1)\n",
    "pred_valid_augmented = lreg_augmented.predict(X_valid_augmented)\n",
    "pred_valid_rf_augmented = rf_augmented.predict(X_valid_augmented)\n",
    "\n",
    "mae_valid_augmented = mean_absolute_error(pred_valid_augmented, Y_valid)\n",
    "print(\"On the validation set, we get %f error for the linear regression\" % mae_valid_augmented)\n",
    "\n",
    "mae_valid_rf_augmented = mean_absolute_error(pred_valid_rf_augmented, Y_valid)\n",
    "print(\"And %f for the random forest regression\" % mae_valid_rf_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Homework\n",
    "\n",
    "**Refactor the code above:**\n",
    "- \"Be lazy. Not just lazy but proactively, agressively lazy\": remove duplication.\n",
    "- Create a single function that takes in data and spits out all success metrics across all of your algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here?\n",
    "\n",
    "\n",
    "- Unigrams (NLTK)\n",
    "- Word vector (gensim, [glove][1], word2vec)\n",
    "- Recurrent neural net\n",
    "- Convolutional neural net\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [Perform sentiment analysis with LSTMS using TensorFlow][2]\n",
    "- [Understanding convolutional neural networks for NLP][3]\n",
    "- [Develop N-Gram multichannel convolutional neural network sentiment analysis][4]\n",
    "\n",
    "[1]: https://nlp.stanford.edu/projects/glove/\n",
    "[2]: https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "[3]: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "[4]: https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
